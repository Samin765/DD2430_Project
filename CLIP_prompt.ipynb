{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samin765/DD2430_Project/blob/main/CLIP_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants\n",
        "Change these to fit your needs"
      ],
      "metadata": {
        "id": "2INOXSFL9vte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path kaggle will download to\n",
        "HM_DATA_PATH = \"/content/drive/MyDrive/dd2430/data/\"\n",
        "\n",
        "# path tourch.save and .load will use\n",
        "PTH_SAVE_PATH = \"/content/drive/MyDrive/dd2430/pth/\"\n",
        "\n",
        "# False if you have already downloaded once\n",
        "DOWNLOAD_FROM_KAGGLE = False\n",
        "\n",
        "# False if you have already created and saved a .pth file to PTH_SAVE_PATH\n",
        "CREATE_NEW_DATASET = False\n",
        "\n",
        "# train, test, val set size. Should sum to 1\n",
        "SET_SIZES = {\n",
        "    \"train\": 0.8,\n",
        "    \"test\": 0.1,\n",
        "    \"val\": 0.1,\n",
        "}"
      ],
      "metadata": {
        "id": "bSDTu4h97TFF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "qpJXUgcT97sz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zPOBAjDlInvv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import inspect #inspect.getsource(transformers.CLIPProcessor)\n",
        "import requests\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import files, drive\n",
        "import gdown\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import PIL.Image\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import transformers\n",
        "from transformers import CLIPModel, CLIPTokenizerFast\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
        "from transformers.models.clip.modeling_clip import (\n",
        "    CLIPTextEmbeddings,\n",
        "    CLIPEncoder,\n",
        "    CLIPTextConfig\n",
        ")\n",
        "\n",
        "from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask, _create_4d_causal_attention_mask\n",
        "from transformers.models.clip.modeling_clip import CLIPOutput, clip_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou5QvXmt3_Kr",
        "outputId": "a1292a65-1122-465c-f9c9-b891632ca9b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "fJAyjnhi-Azx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ePv7xBdq3bPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc59e39-5e00-4b68-b5d2-d72dc037d227"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MbDLguV7cAUV"
      },
      "outputs": [],
      "source": [
        "if DOWNLOAD_FROM_KAGGLE:\n",
        "    !pip install kaggle\n",
        "    files.upload()\n",
        "\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    !cp kaggle.json /root/.kaggle/\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "    %cd HM_DATA_PATH\n",
        "\n",
        "    !kaggle competitions download -c h-and-m-personalized-fashion-recommendations\n",
        "    !unzip -q h-and-m-personalized-fashion-recommendations.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQe6TDNGX4K"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K71-ZF6LGSjM"
      },
      "outputs": [],
      "source": [
        "class HMDataset2(Dataset):\n",
        "    def __init__(self, articles_csv, image_dir, main_class, processor, model, transform=None):\n",
        "        # Load the CSV files\n",
        "        self.articles = pd.read_csv(articles_csv)\n",
        "\n",
        "        # Image directory\n",
        "        self.image_dir = image_dir # image folder\n",
        "        self.processor = processor # prcessor of clip model\n",
        "\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        self.main_class = main_class #for example prod_name\n",
        "        self.id_add = 0 # take next if image does not exist\n",
        "        self.len = self.articles.shape[0]\n",
        "\n",
        "        self.main_classes = self.articles.columns # all classes\n",
        "        self.sub_classes = list(self.articles[self.main_class].unique()) # list of all subclasses\n",
        "        self.count_sub_classes = self.articles[self.main_class].value_counts() # counts in subclasses\n",
        "\n",
        "        print('Max uniform size:', self.articles[self.main_class].value_counts().min()) # <max uniform size\n",
        "\n",
        "        self.class_to_id = {name: i for i, name in enumerate(self.sub_classes)}\n",
        "\n",
        "        self.max_counts = 4 # max number of data per class\n",
        "        self.counts = {name: 0 for name in self.sub_classes} # number of samples each class\n",
        "\n",
        "        self.processor.feature_extractor.do_rescale = False # # make sure image values: False=> [0-1] and True=> [0,255]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "    def get_n_of_each(self, max_counts):\n",
        "        \"\"\"Collects max_counts datapoints from each subclass in large dataset\"\"\"\n",
        "        self.max_counts = max_counts\n",
        "        all_embeds = []\n",
        "        all_labels = []\n",
        "        all_images = []\n",
        "\n",
        "        for idx in range(self.len):\n",
        "          id = self.articles['article_id'][idx]\n",
        "          subclass_name = self.articles[self.main_class][idx]\n",
        "\n",
        "          if self.counts[subclass_name] < self.max_counts:\n",
        "              image_path = f\"{self.image_dir}/0{str(id)[0:2]}/0{id}.jpg\"\n",
        "\n",
        "              try:\n",
        "                  image = Image.open(image_path)\n",
        "\n",
        "                  # get border color\n",
        "                  # only gets the first pixel, but it's good enough\n",
        "                  im_matrix = np.array(image)\n",
        "                  r,g,b = im_matrix[0][0]\n",
        "\n",
        "                  # rezise and add padding\n",
        "                  image.thumbnail((200, 200), Image.LANCZOS)\n",
        "                  padding = (200 - image.size[0], 200 - image.size[1])\n",
        "                  image = ImageOps.expand(image, (padding[0]//2, padding[1]//2, (padding[0]+1)//2, (padding[1]+1)//2), fill=(r,g,b))\n",
        "\n",
        "                  image_tensor = self.transform(image)\n",
        "\n",
        "                  with torch.no_grad():\n",
        "                      image_embeds, processed_images = get_image_emb(self.model, self.processor, image_tensor)\n",
        "\n",
        "                  self.counts[subclass_name]+=1\n",
        "                  all_embeds.append(image_embeds)\n",
        "                  all_labels.append(subclass_name)\n",
        "                  all_images.append(processed_images)\n",
        "              except FileNotFoundError:\n",
        "                  print(f\"Image for article {id} not found. Takes next\")\n",
        "\n",
        "        return torch.cat(all_embeds), all_labels, torch.cat(all_images)\n",
        "\n",
        "\n",
        "\n",
        "class UniformHMDataset(Dataset):\n",
        "    \"\"\"Dataset with perfect class balance\"\"\"\n",
        "    def __init__(self, emb, labels, image):\n",
        "      self.emb = emb\n",
        "      self.labels = labels\n",
        "      self.image = image\n",
        "      self.classes = list(set(labels))\n",
        "      self.class_to_id = {name: i for i, name in enumerate(self.classes)}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.emb[idx], self.labels[idx], self.image[idx]\n",
        "\n",
        "\n",
        "\n",
        "# Needed to create the dataset\n",
        "def get_image_emb(model, processor, images):\n",
        "    \"\"\"Given an tensor of batch images returns the batch image embeddings [batch, 512]\"\"\"\n",
        "\n",
        "    #print(model.vision_model, processor.image_processor)\n",
        "    vision_model = model.vision_model # VIT\n",
        "    image_processor = processor.image_processor # standardise the input\n",
        "    visual_projection = model.visual_projection # fc layer\n",
        "\n",
        "    #standardise, same shape as image\n",
        "    prosessed_images = image_processor(images, return_tensors='pt')['pixel_values']\n",
        "\n",
        "    # apply VIT snd project to latent space  dim [batch, 768]\n",
        "    vision_latent = vision_model(prosessed_images.to(model.device))[1] # not same as text\n",
        "\n",
        "    # project to same dim as text emb by FC layer [batch, 512]\n",
        "    image_embeds = visual_projection(vision_latent)\n",
        "\n",
        "    # normalize so norm is one, good for dot product later\n",
        "    return image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True), prosessed_images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create new dataset\n",
        "This will create a new dataset and save it as a .pth to google drive."
      ],
      "metadata": {
        "id": "3UAgJdAxAOLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XOPl3OzJMadt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c7a6e8-659f-490c-8f31-813e2e992959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max uniform size: 908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image for article 212042043 not found. Takes next\n",
            "Image for article 212042066 not found. Takes next\n",
            "Image for article 241602023 not found. Takes next\n"
          ]
        }
      ],
      "source": [
        "if CREATE_NEW_DATASET:\n",
        "    dataset = HMDataset2(\n",
        "        articles_csv = HM_DATA_PATH + 'articles.csv',\n",
        "        image_dir = HM_DATA_PATH + 'images',\n",
        "        main_class = 'garment_group_name',\n",
        "        model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device),\n",
        "        processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    )\n",
        "\n",
        "    # data per class\n",
        "    n_samples = 10\n",
        "\n",
        "    # you can also set all to n_samples then set the ones you want to 0\n",
        "    for exclude_subclass in ['Unknown', 'Special Offers', 'some other']:\n",
        "        dataset.counts[exclude_subclass]=n_samples\n",
        "\n",
        "    # Create uniform dataset\n",
        "    image_emb, labels, images = dataset.get_n_of_each(n_samples)\n",
        "\n",
        "    data_to_save = {\n",
        "        'image_embedding': image_emb,\n",
        "        'class_text': labels,\n",
        "        'images': images,\n",
        "    }\n",
        "\n",
        "    os.makedirs(PTH_SAVE_PATH, exist_ok=True)\n",
        "    torch.save(data_to_save, f'{PTH_SAVE_PATH}HM_data_{n_samples}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from Google Drive\n",
        "If you already have the HM data and .pth saved in google drive, this is where the actual code/program begins.\n",
        "\n",
        "Remember to change the constants at the top so you don't download and/or create a new dataset next time."
      ],
      "metadata": {
        "id": "0qGqNs0B_Ek-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P3U1Tr3yMHGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1804a81e-4cab-4e56-d940-5a291f75443a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-9d34e0df320c>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_data = torch.load(f'{PTH_SAVE_PATH}{file_to_load}')\n"
          ]
        }
      ],
      "source": [
        "n_samples = 10\n",
        "file_to_load = f\"HM_data_{n_samples}.pth\"\n",
        "\n",
        "loaded_data = torch.load(f'{PTH_SAVE_PATH}{file_to_load}')\n",
        "\n",
        "image_emb = loaded_data['image_embedding']\n",
        "labels = loaded_data['class_text']\n",
        "images = loaded_data['images']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into train, test, and val set\n",
        "Use `dataset_train`, `dataset_test`, and `dataset_val`."
      ],
      "metadata": {
        "id": "5H_a8067FDEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing the data in equal parts to the three sets\n",
        "combined = sorted(zip(labels, image_emb, images), key=lambda x: x[0])\n",
        "labels, image_emb, images = zip(*combined)\n",
        "\n",
        "train_labels, train_image_emb, train_images = [], [], []\n",
        "test_labels, test_image_emb, test_images = [], [], []\n",
        "val_labels, val_image_emb, val_images = [], [], []\n",
        "\n",
        "for i in range(0, len(combined) - 1, n_samples):\n",
        "    labels_sub = labels[i : i + n_samples]\n",
        "    image_emb_sub = image_emb[i : i + n_samples]\n",
        "    images_sub = images[i : i + n_samples]\n",
        "\n",
        "    s = lambda t: int(float(len(labels_sub)) * SET_SIZES[t])\n",
        "\n",
        "    train_labels.extend(labels_sub[:s(\"train\")])\n",
        "    train_image_emb.extend(image_emb_sub[:s(\"train\")])\n",
        "    train_images.extend(images_sub[:s(\"train\")])\n",
        "\n",
        "    test_labels.extend(labels_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "    test_image_emb.extend(image_emb_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "    test_images.extend(images_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "\n",
        "    val_labels.extend(labels_sub[s(\"train\") + s(\"test\"):])\n",
        "    val_image_emb.extend(image_emb_sub[s(\"train\") + s(\"test\"):])\n",
        "    val_images.extend(images_sub[s(\"train\") + s(\"test\"):])\n",
        "\n",
        "# shuffle the data in each set\n",
        "def shuffle_set(labels, image_emb, images):\n",
        "    combined = list(zip(labels, image_emb, images))\n",
        "    random.shuffle(combined)\n",
        "    return zip(*combined)\n",
        "\n",
        "train_labels, train_image_emb, train_images = shuffle_set(train_labels, train_image_emb, train_images)\n",
        "test_labels, test_image_emb, test_images = shuffle_set(test_labels, test_image_emb, test_images)\n",
        "val_labels, val_image_emb, val_images = shuffle_set(val_labels, val_image_emb, val_images)\n",
        "\n",
        "# create the datasets\n",
        "dataset_train = UniformHMDataset(train_image_emb, train_labels, train_images)\n",
        "dataset_test = UniformHMDataset(test_image_emb, test_labels, test_images)\n",
        "dataset_val = UniformHMDataset(val_image_emb, val_labels, val_images)\n",
        "\n",
        "# checking\n",
        "print(len(labels), len(dataset_train.labels), len(dataset_test.labels), len(dataset_val.labels))\n",
        "print(dataset_train.labels)"
      ],
      "metadata": {
        "id": "HQQK-U0FFCoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5f2958-f979-4f74-cc85-6604445ce78f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190 152 19 19\n",
            "('Dresses Ladies', 'Trousers', 'Shirts', 'Swimwear', 'Dresses/Skirts girls', 'Blouses', 'Accessories', 'Skirts', 'Dresses Ladies', 'Trousers Denim', 'Jersey Fancy', 'Accessories', 'Woven/Jersey/Knitted mix Baby', 'Skirts', 'Outdoor', 'Dresses Ladies', 'Skirts', 'Accessories', 'Dresses Ladies', 'Under-, Nightwear', 'Knitwear', 'Under-, Nightwear', 'Accessories', 'Socks and Tights', 'Shorts', 'Under-, Nightwear', 'Woven/Jersey/Knitted mix Baby', 'Accessories', 'Shoes', 'Jersey Fancy', 'Shirts', 'Dresses Ladies', 'Dresses/Skirts girls', 'Trousers', 'Knitwear', 'Trousers Denim', 'Shorts', 'Knitwear', 'Dresses/Skirts girls', 'Outdoor', 'Under-, Nightwear', 'Jersey Basic', 'Dressed', 'Shirts', 'Trousers', 'Socks and Tights', 'Woven/Jersey/Knitted mix Baby', 'Under-, Nightwear', 'Dresses Ladies', 'Woven/Jersey/Knitted mix Baby', 'Blouses', 'Trousers', 'Swimwear', 'Dressed', 'Swimwear', 'Jersey Fancy', 'Dresses Ladies', 'Jersey Fancy', 'Outdoor', 'Knitwear', 'Dresses Ladies', 'Blouses', 'Knitwear', 'Trousers', 'Trousers Denim', 'Skirts', 'Under-, Nightwear', 'Accessories', 'Shirts', 'Jersey Fancy', 'Jersey Basic', 'Shirts', 'Swimwear', 'Shoes', 'Accessories', 'Jersey Basic', 'Knitwear', 'Jersey Basic', 'Jersey Fancy', 'Shorts', 'Woven/Jersey/Knitted mix Baby', 'Skirts', 'Swimwear', 'Dressed', 'Shorts', 'Socks and Tights', 'Socks and Tights', 'Knitwear', 'Socks and Tights', 'Woven/Jersey/Knitted mix Baby', 'Dresses/Skirts girls', 'Trousers Denim', 'Under-, Nightwear', 'Trousers', 'Blouses', 'Shorts', 'Skirts', 'Socks and Tights', 'Woven/Jersey/Knitted mix Baby', 'Shoes', 'Shoes', 'Blouses', 'Dressed', 'Shirts', 'Shorts', 'Trousers', 'Outdoor', 'Trousers Denim', 'Dresses/Skirts girls', 'Jersey Basic', 'Swimwear', 'Dressed', 'Dressed', 'Knitwear', 'Blouses', 'Skirts', 'Dressed', 'Outdoor', 'Shoes', 'Woven/Jersey/Knitted mix Baby', 'Blouses', 'Jersey Basic', 'Shorts', 'Dresses/Skirts girls', 'Jersey Fancy', 'Outdoor', 'Skirts', 'Trousers Denim', 'Shoes', 'Socks and Tights', 'Under-, Nightwear', 'Swimwear', 'Socks and Tights', 'Shoes', 'Outdoor', 'Dresses/Skirts girls', 'Shoes', 'Dressed', 'Trousers Denim', 'Shirts', 'Outdoor', 'Accessories', 'Jersey Basic', 'Jersey Fancy', 'Jersey Basic', 'Trousers Denim', 'Swimwear', 'Blouses', 'Trousers', 'Shirts', 'Dresses/Skirts girls', 'Shorts')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xduBJBBGS_b"
      },
      "source": [
        "#Coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CbkvjOybM5D1"
      },
      "outputs": [],
      "source": [
        "def show_images(X):\n",
        "    \"\"\"Given images as tensor shows them\"\"\"\n",
        "    np_images = X.numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    grid_img = torchvision.utils.make_grid(torch.tensor(np_images))\n",
        "    plt.imshow(np.transpose(grid_img, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def return_normal(tensor_image, processor, ant, plot):\n",
        "    \"\"\"Reverse normalization and then show image, looks weird otherwise\"\"\"\n",
        "    mean = torch.tensor(processor.feature_extractor.image_mean).view(1, 3, 1, 1)# only colour channels\n",
        "    std = torch.tensor(processor.feature_extractor.image_std).view(1, 3, 1, 1)\n",
        "    tensor_image = tensor_image.cpu() * std + mean  # Add back the normalization\n",
        "    if plot:\n",
        "      show_images(tensor_image[0:ant,:,:,:])# do not show all\n",
        "    return tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xePBORGTNoWc"
      },
      "outputs": [],
      "source": [
        "def get_text_emb(model, processor, text):\n",
        "    \"\"\"Given an tensor of batch text returns the batch text embeddings [batch, 512],\n",
        "    define X as the number of tokens and might differ from text length\"\"\"\n",
        "    #print(model.text_model, processor.tokenizer)\n",
        "    text_model = model.text_model # VIT\n",
        "    text_tokenizer = processor.tokenizer # tokenize the input\n",
        "    text_projection = model.text_projection # fc layer\n",
        "    #tokenize, returns 2 tensors, tokens and attention mask [batch, X]\n",
        "    tokenized_text = text_tokenizer(text, return_tensors='pt', padding=True, truncation = True)\n",
        "    # apply TRANSFORMER and project to latent space  dim [batch, 512]\n",
        "    text_latent = text_model(**tokenized_text.to(model.device))[1]\n",
        "    # project to same dim as text emb by FC layer [batch, 512] unneccessary???\n",
        "    text_embeds = text_projection(text_latent) #[batch, 512] to [batch, 512] same\n",
        "    # TODO add LORA\n",
        "    # normalize so norm is one, good for dot product later\n",
        "    return text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "def apply_clip(text_embeds, image_embeds, model, train=False):\n",
        "    \"\"\"Forward pass of clip\"\"\"\n",
        "    device = model.device # use gpu\n",
        "    logit_scale = model.logit_scale.exp().to(device) # temperature param\n",
        "    text_embeds, image_embeds = text_embeds.to(device), image_embeds.to(device)\n",
        "    # TODO add LORA\n",
        "    logits_per_image = torch.matmul(image_embeds, text_embeds.t()) * logit_scale\n",
        "    loss = 0\n",
        "    if train: # must have same ammount of text as images for training\n",
        "        loss = clip_loss(logits_per_image.t())\n",
        "    return logits_per_image, loss\n",
        "\n",
        "def get_text_emb_soft(model, processor, text, soft_prompt_hidden):\n",
        "    \"\"\"Just like get_text_emb but for sof prompts,\n",
        "    define X as the number of tokens and might differ from text length\"\"\"\n",
        "    #print(model.text_model, processor.tokenizer)\n",
        "    text_model = model.text_model # VIT original\n",
        "    text_tokenizer = processor.tokenizer # tokenize the input\n",
        "    text_projection = model.text_projection # fc layer\n",
        "    text_embedder_inner = text_model.embeddings # OBS this is the inner embedding NOT the one we want\n",
        "    # tokenize the text, returns 2 tensors, tokens and attention mask [batch,X+soft]# token len not same as text\n",
        "    tokenized_text = text_tokenizer(text, return_tensors='pt', padding=True, truncation = True) # returns tokens and attention mask\n",
        "    # add soft prompt--------------\n",
        "    # Take out the parts\n",
        "    input_ids, attention_mask = tokenized_text['input_ids'].to(device), tokenized_text['attention_mask'].to(device)\n",
        "    attention_mask = attention_mask\n",
        "    # get only hiddden states, this is before textTransformer is applied\n",
        "    hidden_states= text_embedder_inner(input_ids) #torch.Size([batch_size, X, 512])\n",
        "    batch_size = hidden_states.size(0)\n",
        "    # adding vectors to the embedding torch.Size([4, X+softprompts, 512])\n",
        "    expand_hidden = soft_prompt_hidden.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "    hidden_states = torch.cat([expand_hidden.to(device), hidden_states], dim=1)\n",
        "    # must match the shape\n",
        "    soft_prompt_attention_mask = torch.ones(batch_size, soft_prompt_hidden.shape[0], dtype=attention_mask.dtype)\n",
        "    attention_mask = torch.cat([soft_prompt_attention_mask.to(device), attention_mask], dim=1)# just ones\n",
        "    #end of soft prompt--------------\n",
        "    # apply costum transformer snd project to latent space  dim [batch, 512]\n",
        "    text_latent = forward_text(input_ids, attention_mask, hidden_states, text_model)\n",
        "    # project to same dim as text emb by FC layer [batch, 512] unneccessary???\n",
        "    text_embeds = text_projection(text_latent) #[batch, 512] to [batch, 512] same\n",
        "    # TODO add LORA\n",
        "    # normalize so norm is one, good for dot product later\n",
        "    return text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "def forward_text(input_ids, attention_mask, hidden_states, text_model):\n",
        "    \"\"\"Modified forward pass of the text model TRANSFORMER to include soft prompts\"\"\"\n",
        "    #print(text_model) # prints the architecture\n",
        "    num_soft = hidden_states.shape[1]-input_ids.shape[1]\n",
        "    input_shape = input_ids.size()\n",
        "\n",
        "    causal_attention_mask = _create_4d_causal_attention_mask((hidden_states.shape[0], hidden_states.shape[1])\n",
        "    , hidden_states.dtype, device=hidden_states.device)\n",
        "    if attention_mask is not None and not text_model._use_flash_attention_2:\n",
        "        attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n",
        "\n",
        "    encoder_outputs = text_model.encoder(\n",
        "        inputs_embeds=hidden_states,\n",
        "        attention_mask=attention_mask,\n",
        "        causal_attention_mask=causal_attention_mask)\n",
        "    last_hidden_state = encoder_outputs[0]\n",
        "    last_hidden_state = text_model.final_layer_norm(last_hidden_state)\n",
        "    last_hidden_state = last_hidden_state[:, num_soft:, :] # remove prompt states\n",
        "    pooled_output = last_hidden_state[\n",
        "        torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
        "        input_ids.view(-1, input_shape[-1]).to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),]\n",
        "    return pooled_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "6fClfPP_3FRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t5XtPmt8Po07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ff661e-8632-4e6c-8589-cec7bb321988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor.feature_extractor.do_rescale = False # make sure image values: False=> [0-1] and True=> [0,255]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kYHe1mXGSQ65"
      },
      "outputs": [],
      "source": [
        "num_soft_prompts = 3\n",
        "soft_prompts = nn.Parameter(torch.zeros(num_soft_prompts, model.text_projection.in_features), requires_grad=True)\n",
        "#randn or zeros\n",
        "optimizer = torch.optim.Adam([soft_prompts], lr=1e-3) # note that we here specify soft_prompts to tune\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "dataloader = DataLoader(dataset, batch_size=40, shuffle=False) # do not shuffle for eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7saJriyUo_J"
      },
      "source": [
        "**Baseline model and untuned softprompt**\n",
        "\n",
        "\n",
        "obs,compare so that the modified transformer gets same result if num_soft = 0 yes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4dNiZwM6PpuL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d7c4a276-5b35-4508-ce0a-92419ae3417b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2639 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Subclasses of Dataset should implement __getitem__.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f26b6a0fbb36>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_nr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#text = ['An image of a '+i for i in labels] # for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT_co\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subclasses of Dataset should implement __getitem__.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# def __getitems__(self, indices: List) -> List[T_co]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
          ]
        }
      ],
      "source": [
        "show_image = True\n",
        "all_predictions_baseline = []\n",
        "all_predictions_soft_untuned = []\n",
        "all_labels= []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader)):\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        #text = ['An image of a '+i for i in labels] # for training\n",
        "        #image_embeds, _ = get_image_emb(model, processor, return_normal(images, processor, 0, False)) #use or not\n",
        "        # baseline\n",
        "        text_embeds = get_text_emb(model, processor, text)\n",
        "        logits_per_image1, loss = apply_clip(text_embeds, image_embeds, model)\n",
        "        probs1 = logits_per_image1.softmax(dim=-1).cpu().numpy()\n",
        "        # softprompts\n",
        "\n",
        "        text_embeds = get_text_emb_soft(model, processor, text, soft_prompts)\n",
        "\n",
        "        logits_per_image2, loss = apply_clip(text_embeds, image_embeds, model)\n",
        "        probs2 = logits_per_image2.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        if show_image and batch_nr%40==0:\n",
        "            images = return_normal(images, processor, 4, True)\n",
        "            #print(dataset.classes)\n",
        "            #print(probs1.round(decimals=3))\n",
        "            #print('Base, then soft')\n",
        "            #print(probs2.round(decimals=3))\n",
        "\n",
        "        predicted_class_base = logits_per_image1.argmax(dim=-1)\n",
        "        predicted_class_soft_untuned = logits_per_image2.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_baseline.append(predicted_class_base)\n",
        "        all_predictions_soft_untuned.append(predicted_class_soft_untuned)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "\n",
        "all_predictions_baseline=torch.cat(all_predictions_baseline).cpu()\n",
        "all_predictions_soft_untuned=torch.cat(all_predictions_soft_untuned).cpu()\n",
        "\n",
        "print(all_labels)\n",
        "print(all_predictions_baseline)\n",
        "\n",
        "correct_base = all_predictions_baseline==torch.tensor(all_labels).cpu()\n",
        "correct_soft_unt = all_predictions_soft_untuned==torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy baseline {100*correct_base.sum()/correct_base.shape[0]} %')\n",
        "print(f'Accuracy Soft prompt untuned {100*correct_soft_unt.sum()/correct_soft_unt.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJThFnDiVAJV"
      },
      "source": [
        "**Finetuning**\n",
        "\n",
        "Performance decreases if we add untuned soft prompts, now we finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZZDbNteU_VT"
      },
      "outputs": [],
      "source": [
        "show_image = True\n",
        "#print(model)#https://github.com/search?q=repo%3Ahuggingface%2Ftransformers%20CLIPModel&type=code\n",
        "model.train()\n",
        "loss_list = []\n",
        "batch_size = 40\n",
        "epochs = 400\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        images.to(model.device)\n",
        "        #text = ['An image of a '+i for i in dataset.classes] # for prediction, can use what we want\n",
        "        text = [''+i for i in labels] # for training, must use 1-1 map\n",
        "        #image_embeds, _ = get_image_emb(model, processor, return_normal(images, processor, 0, False)) #SLOW\n",
        "        text_embeds = get_text_emb_soft(model, processor, text, soft_prompts)\n",
        "        logits_per_image, loss = apply_clip(text_embeds, image_embeds, model, train=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(soft_prompts.grad) to see that back prop works, is none otherwise\n",
        "        running_loss +=loss.item()\n",
        "    loss_list.append(running_loss)\n",
        "\n",
        "torch.save(soft_prompts, 'soft_prompts.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA1lU0jfCadS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(range(1, epochs+1)), loss_list, label='Training Loss')\n",
        "# Adding labels and title\n",
        "plt.title('Training Loss Over Datapoints')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate performance**"
      ],
      "metadata": {
        "id": "PfBohkuM3f0H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQYLnTuGto_"
      },
      "outputs": [],
      "source": [
        "all_labels= []\n",
        "all_predictions_soft = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader)):\n",
        "        text = [''+i for i in dataset.classes] # for eval\n",
        "        #image_embeds, _ = get_image_emb(model, processor, return_normal(images, processor, 0, False)) #use or not\n",
        "        #text_embeds = get_text_emb_soft(model, processor, text, soft_prompts)\n",
        "        text_embeds = get_text_emb_soft(model, processor, text, soft_prompts)\n",
        "        logits_per_image2, loss = apply_clip(text_embeds, image_embeds, model)\n",
        "        predicted_class_soft = logits_per_image2.argmax(dim=-1)\n",
        "        all_predictions_soft.append(predicted_class_soft)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "all_predictions_soft=torch.cat(all_predictions_soft).cpu()\n",
        "correct_soft = all_predictions_soft==torch.tensor(all_labels).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nOf {max(all_labels)} classes random is {1/max(all_labels)}%')\n",
        "print(f'Accuracy baseline {100*correct_base.sum()/correct_base.shape[0]} %')\n",
        "print(f'Accuracy Soft prompt untuned {100*correct_soft_unt.sum()/correct_soft_unt.shape[0]} %')\n",
        "print(f'Accuracy Soft prompt {100*correct_soft.sum()/correct_soft.shape[0]} %')\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_predictions_soft, all_labels))\n",
        "print(dataset.class_to_id)"
      ],
      "metadata": {
        "id": "6HCFdhqwG6Zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}