{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1309ae9-762c-4350-ab22-c361705f1f17",
   "metadata": {},
   "source": [
    "## Code to transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae25764a-2c56-483c-99ef-8321a7ab6cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e8ca33c8-4723-4410-8315-d9d79e0903d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import transformers, torch\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "587bf580-3b89-4e46-ba66-a4d36d18859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import model_functions, utils, training, datasets\n",
    "def update():# if you change our files\n",
    "    import model_functions, utils, training, datasets\n",
    "    for lib in [model_functions, utils, training, datasets]:\n",
    "        importlib.reload(lib)# issues with not updating\n",
    "update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "24269ddc-4711-4707-aca9-7326976b3311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available(): # For apple silicon\n",
    "    device = 'mps'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec63324-0058-4c99-af91-4345d10052f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./HM_DATA_PATH/images/..., ./HM_DATA_PATH/articles.csv, etc\n",
    "HM_DATA_PATH = \"./data/\"\n",
    "\n",
    "# path where new dataset will be saved to\n",
    "HM_DATA_PATH_NEW = \"./dataset/\"\n",
    "\n",
    "SRC_IMAGE_DIR = HM_DATA_PATH + \"images\"\n",
    "DEST_IMAGE_DIR = HM_DATA_PATH_NEW + \"images\"\n",
    "\n",
    "DEST_CSV = HM_DATA_PATH_NEW + \"articles.csv\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "if not os.path.exists(DEST_IMAGE_DIR):\n",
    "    print(f\"Directory does not exist: {DEST_IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "74b1de61-5e01-4cce-b9d9-97e169acabcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/jupyter/DD2430_Project/src\n"
     ]
    }
   ],
   "source": [
    "def get_number_image_ids():\n",
    "    \"\"\"Loop trough all files in the folder and get image_ids from name\"\"\"\n",
    "    file_names =[]\n",
    "    for root, _, files in os.walk(DEST_IMAGE_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):# article id\n",
    "                if len(file[1:-4])==9:\n",
    "                    file_names.append(int(file[1:-4]))\n",
    "    return file_names\n",
    "\n",
    "def create_folders():#markus\n",
    "    if not os.path.exists(DEST_IMAGE_DIR):\n",
    "        os.makedirs(DEST_IMAGE_DIR)\n",
    "\n",
    "\n",
    "def copy_csv_json_files(all_files=False):#markus\n",
    "    # only copy articles.csv\n",
    "    shutil.copy2(HM_DATA_PATH + \"articles.csv\", HM_DATA_PATH_NEW + \"articles.csv\")\n",
    "\n",
    "    # only copy over .csv and .json\n",
    "    if all_files:\n",
    "        for file in os.listdir(HM_DATA_PATH):\n",
    "            if file.endswith(\".csv\") or file.endswith(\".json\"):\n",
    "                src = HM_DATA_PATH + file\n",
    "                dest = HM_DATA_PATH_NEW + file\n",
    "\n",
    "                os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "                shutil.copy2(src, dest)\n",
    "\n",
    "\n",
    "def images_processing(): #markus\n",
    "    # go through all images\n",
    "    for root, _, files in os.walk(SRC_IMAGE_DIR):\n",
    "        for file in tqdm(files):\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    img = Image.open(img_path)\n",
    "\n",
    "                    # when this works it's good. Otherwise it's bad.\n",
    "\n",
    "                    # save rgb of all pixels of border of image\n",
    "                    # np_image = np.array(img)\n",
    "\n",
    "                    # top_border = np_image[1, 1:-1, :]\n",
    "                    # bottom_border = np_image[-2, 1:-1, :]\n",
    "                    # left_border = np_image[1:-1, 1, :]\n",
    "                    # right_border = np_image[1:-1, -2, :]\n",
    "\n",
    "                    # border_pixels = np.concatenate([top_border, bottom_border, left_border, right_border], axis=0)\n",
    "\n",
    "                    # def get_brightest_or_closest_to_white(border_pixels):\n",
    "                    #     def distance_to_white(pixel):\n",
    "                    #         r, g, b = pixel\n",
    "                    #         return np.sqrt((255 - r) ** 2 + (255 - g) ** 2 + (255 - b) ** 2)\n",
    "                    \n",
    "                    #     # Find the pixel with the highest brightness (or closest to white)\n",
    "                    #     brightest_pixel = min(border_pixels, key=distance_to_white)\n",
    "                    #     return tuple(brightest_pixel)\n",
    "\n",
    "                    # Choose the brightest or closest to white color for padding\n",
    "                    # padding_color = get_brightest_or_closest_to_white(border_pixels)\n",
    "\n",
    "\n",
    "                    # just pick RGB of background manually and assign to every image.\n",
    "                    padding_color = (236, 235, 233)\n",
    "                    r,g,b = padding_color\n",
    "\n",
    "                    # rezise and add padding\n",
    "                    square_width_height = 224 # transformer take 224x224 image res\n",
    "                    img.thumbnail((square_width_height, square_width_height), Image.LANCZOS)\n",
    "                    padding = (square_width_height - img.size[0], square_width_height - img.size[1])\n",
    "                    img = ImageOps.expand(img, (padding[0]//2, padding[1]//2, (padding[0]+1)//2, (padding[1]+1)//2), fill=(r,g,b))\n",
    "\n",
    "                    # save new image\n",
    "                    output_path = os.path.join(DEST_IMAGE_DIR, file)\n",
    "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                    img.save(output_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                    \n",
    "def downscale_folders():#markus\n",
    "    create_folders()\n",
    "    copy_csv_json_files()\n",
    "    images_processing()\n",
    "#downscale_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7250e5d-6d8f-4c1d-a672-9dad5d9e5c00",
   "metadata": {},
   "source": [
    "**Load all data into tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "fdeb2d9a-9433-4b3d-ad01-39ecce51f344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105542 105099\n",
      "443 are missing\n",
      "105099 443\n",
      "105099\n"
     ]
    }
   ],
   "source": [
    "image_ids = get_number_image_ids()\n",
    "df = pd.read_csv(DEST_CSV)\n",
    "article_id = df['article_id'].values\n",
    "print(len(article_id),len(image_ids))\n",
    "print(len(article_id)-len(image_ids), 'are missing')\n",
    "common_ids = set(article_id).intersection(image_ids)\n",
    "different_ids = set(article_id).difference(image_ids)\n",
    "print(len(common_ids), len(different_ids))\n",
    "\n",
    "filtered_df = df[~df['article_id'].isin(different_ids)]\n",
    "print(len(filtered_df))\n",
    "filtered_df.to_csv(HM_DATA_PATH_NEW+'articles_filtered.csv', index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395ab6c-cfe5-4345-9930-3b99c23f36da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor.feature_extractor.do_rescale = False # make sure image values: False=> [0-1] and True=> [0,255]\n",
    "clip = {'m':model, 'p':processor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e75f40-48be-437c-9c70-5574cfab27aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 105099/105099 [19:39<00:00, 89.12Images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105099, 512]) torch.Size([105099])\n",
      "Size of PyTorch tensor: 215242752 bytes\n"
     ]
    }
   ],
   "source": [
    "import model_functions\n",
    "def get_data_as_tensors(path, clip, limit =10): # takes 20min for all\n",
    "    \"\"\"Loops trough all folder and returns a 0.5 GB tensor of emb for all 100k images,  and a label of input_ids\"\"\"\n",
    "    labels, images, embeds, count = torch.zeros(limit, dtype=torch.int32), [],torch.zeros((limit,512), dtype=torch.float32), 0\n",
    "    transform = transforms.ToTensor()\n",
    "    with tqdm(total=limit, desc=\"Loading\", unit=\"Images\") as pbar:\n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    article_id_im = file[1:-4]\n",
    "                    if len(article_id_im)==9 and count<limit: # article id \n",
    "                        img_path = os.path.join(root, file)\n",
    "                        image = transform(Image.open(img_path)).to(device)\n",
    "                        # images.append(image)\n",
    "                        labels[count]=int(article_id_im)\n",
    "                        with torch.no_grad():\n",
    "                            image_emb, _ = model_functions.get_image_emb(model, processor, image, normalize=True)\n",
    "                        embeds[count]=(image_emb.squeeze()).cpu()\n",
    "                        count +=1\n",
    "                        del image\n",
    "                        del image_emb\n",
    "                        del _\n",
    "                        torch.cuda.empty_cache() \n",
    "                        pbar.update(1)\n",
    "    return embeds, labels #[X, 3, 224, 224]\n",
    "    # full image tensor is [100000,3,224,224] thus 55 GB not possible to fit in ram\n",
    "    #2048 Byte per emb thus 0.2GB for full ([X, 512])\n",
    "    \n",
    "embeds, labels = get_data_as_tensors(DEST_IMAGE_DIR, clip, len(common_ids))\n",
    "print(embeds.shape, labels.shape)\n",
    "def bytes_tensor(tensor):\n",
    "    size_in_bytes = tensor.element_size() * tensor.numel()\n",
    "    print(f\"Size of PyTorch tensor: {size_in_bytes} bytes\")\n",
    "\n",
    "torch.save(embeds, HM_DATA_PATH_NEW+'embedds.pth')\n",
    "torch.save(labels, HM_DATA_PATH_NEW+'labels.pth')\n",
    "\n",
    "bytes_tensor(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4abde046-355f-4950-9a2e-a4241343c4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105099\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(HM_DATA_PATH_NEW+'articles_filtered.csv')\n",
    "embs = torch.load(HM_DATA_PATH_NEW+'embedds.pth', weights_only=True)\n",
    "labs = torch.load(HM_DATA_PATH_NEW+'labels.pth', weights_only=True).tolist()\n",
    "print(len(labs))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
