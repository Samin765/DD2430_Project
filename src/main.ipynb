{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2INOXSFL9vte"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bSDTu4h97TFF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# False if you have already created and saved a .pth file to PTH_SAVE_PATH\n",
    "CREATE_NEW_DATASET = True\n",
    "\n",
    "# train, test, val set size. Should sum to 1\n",
    "SET_SIZES = {\n",
    "    \"train\": 0.8,\n",
    "    \"test\": 0.1,\n",
    "    \"val\": 0.1,\n",
    "}\n",
    "\n",
    "# samples per class in uniform dataset\n",
    "N_SAMPLES = 250\n",
    "\n",
    "# path to dataset (do not change)\n",
    "HM_DATA_PATH = \"../dataset/\"\n",
    "\n",
    "# path to pth saves (do not change)\n",
    "PTH_SAVE_PATH = \"../pth/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpJXUgcT97sz"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zPOBAjDlInvv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/DD2430_Project/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "\n",
    "# Our own files\n",
    "print(os.getcwd())\n",
    "# sys.path.append('./src/')\n",
    "import model_functions\n",
    "import utils\n",
    "import training\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou5QvXmt3_Kr",
    "outputId": "ee73a633-1d13-47ed-86e3-640c1a03e48d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available(): # For apple silicon\n",
    "    device = 'mps'\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJAyjnhi-Azx"
   },
   "source": [
    "# Download data\n",
    "Only run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RAoFqqa2E_XB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1EZ3AfRp-RMj70qZzIAC-BR0sHsrfjOWx\n",
      "From (redirected): https://drive.google.com/uc?id=1EZ3AfRp-RMj70qZzIAC-BR0sHsrfjOWx&confirm=t&uuid=5842bef9-9163-4b29-996a-153b02db3a69\n",
      "To: /home/jupyter/DD2430_Project/src/dataset.zip\n",
      "100%|██████████| 431M/431M [00:02<00:00, 197MB/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(HM_DATA_PATH):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1EZ3AfRp-RMj70qZzIAC-BR0sHsrfjOWx\")\n",
    "    !unzip -q dataset.zip -d ../\n",
    "    !rm -fr dataset.zip\n",
    "else:\n",
    "    print(\"Data already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdQe6TDNGX4K"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UAgJdAxAOLv"
   },
   "source": [
    "## Create new dataset\n",
    "This will create a new dataset and save it as a .pth to google drive. If you getan error, then it is most likely becuase you can not make a daatset that large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOPl3OzJMadt",
    "outputId": "d283740f-11f5-4eeb-da55-fdd4b0ebea25",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max uniform size: 908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image for article 179208001 not found. Takes next\n",
      "Image for article 212629004 not found. Takes next\n",
      "Image for article 215324023 not found. Takes next\n",
      "Image for article 216961011 not found. Takes next\n",
      "Image for article 272591001 not found. Takes next\n",
      "Image for article 348657006 not found. Takes next\n",
      "Image for article 369423002 not found. Takes next\n",
      "Image for article 388916001 not found. Takes next\n",
      "Image for article 397376010 not found. Takes next\n",
      "Image for article 398947001 not found. Takes next\n",
      "Image for article 408875001 not found. Takes next\n",
      "Image for article 420264002 not found. Takes next\n",
      "Image for article 425683012 not found. Takes next\n",
      "Image for article 442786001 not found. Takes next\n",
      "Image for article 468666002 not found. Takes next\n",
      "Image for article 475827007 not found. Takes next\n",
      "Image for article 480076004 not found. Takes next\n",
      "Image for article 481777003 not found. Takes next\n",
      "Image for article 481797022 not found. Takes next\n",
      "Image for article 484864002 not found. Takes next\n",
      "Image for article 487926002 not found. Takes next\n",
      "Image for article 489758004 not found. Takes next\n",
      "Image for article 490275001 not found. Takes next\n",
      "Image for article 496762004 not found. Takes next\n",
      "Image for article 504152001 not found. Takes next\n",
      "Image for article 504960001 not found. Takes next\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mcounts[exclude_subclass]\u001b[38;5;241m=\u001b[39mn_samples\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create uniform dataset\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m image_emb, labels, images \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_n_of_each\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m data_to_save \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m: image_emb,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_text\u001b[39m\u001b[38;5;124m'\u001b[39m: labels,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: images,\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save as .pth\u001b[39;00m\n",
      "File \u001b[0;32m~/DD2430_Project/src/datasets.py:77\u001b[0m, in \u001b[0;36mHMDataset2.get_n_of_each\u001b[0;34m(self, max_counts)\u001b[0m\n\u001b[1;32m     74\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 77\u001b[0m     image_embeds, processed_images \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_emb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounts[subclass_name] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     81\u001b[0m all_embeds\u001b[38;5;241m.\u001b[39mappend(image_embeds)\n",
      "File \u001b[0;32m~/DD2430_Project/src/model_functions.py:82\u001b[0m, in \u001b[0;36mget_image_emb\u001b[0;34m(model, processor, images, normalize)\u001b[0m\n\u001b[1;32m     79\u001b[0m visual_projection \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvisual_projection  \u001b[38;5;66;03m# fc layer\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# standardise, same shape as image\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m prosessed_images \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# apply VIT snd project to latent space  dim [batch, 768]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m vision_latent \u001b[38;5;241m=\u001b[39m vision_model(prosessed_images\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice))[\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# not same as text\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:325\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 325\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    328\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:191\u001b[0m, in \u001b[0;36mCLIPImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize must contain either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m output_size \u001b[38;5;241m=\u001b[39m get_resize_output_image_size(\n\u001b[1;32m    186\u001b[0m     image,\n\u001b[1;32m    187\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m    188\u001b[0m     default_to_square\u001b[38;5;241m=\u001b[39mdefault_to_square,\n\u001b[1;32m    189\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_transforms.py:332\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    331\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m--> 332\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_transforms.py:210\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale, input_data_format)\u001b[0m\n\u001b[1;32m    207\u001b[0m     image \u001b[38;5;241m=\u001b[39m rescale(image, \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    209\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3297\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3297\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3298\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtostring\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   3299\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_DATASET:\n",
    "    dataset = datasets.HMDataset2(\n",
    "        articles_csv = HM_DATA_PATH + 'articles.csv',\n",
    "        image_dir = HM_DATA_PATH + 'images',\n",
    "        main_class = 'garment_group_name',\n",
    "        model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device),\n",
    "        processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    )\n",
    "    # data per class\n",
    "    n_samples =  N_SAMPLES\n",
    "\n",
    "    assert dataset.articles[dataset.main_class].value_counts().min()>n_samples, 'Can not make balanced set'\n",
    "\n",
    "    # you can also set all to n_samples then set the ones you want to 0\n",
    "    for exclude_subclass in ['Unknown', 'Special Offers', 'some other']:\n",
    "        dataset.counts[exclude_subclass]=n_samples\n",
    "\n",
    "    # Create uniform dataset\n",
    "    image_emb, labels, images = dataset.get_n_of_each(n_samples)\n",
    "\n",
    "    data_to_save = {\n",
    "        'image_embedding': image_emb,\n",
    "        'class_text': labels,\n",
    "        'images': images,\n",
    "    }\n",
    "    \n",
    "    # Save as .pth\n",
    "    os.makedirs(PTH_SAVE_PATH, exist_ok=True)\n",
    "    torch.save(data_to_save, f\"{PTH_SAVE_PATH}HM_data_{n_samples}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qGqNs0B_Ek-"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3U1Tr3yMHGb",
    "outputId": "969a308e-29d3-48d5-9f08-74a3ea0e9eaf"
   },
   "outputs": [],
   "source": [
    "file_to_load = f\"HM_data_{N_SAMPLES}.pth\"\n",
    "\n",
    "loaded_data = torch.load(f'{PTH_SAVE_PATH}{file_to_load}')\n",
    "\n",
    "image_emb = loaded_data['image_embedding']\n",
    "labels = loaded_data['class_text']\n",
    "images = loaded_data['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fClfPP_3FRt",
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJThFnDiVAJV"
   },
   "source": [
    "**Finetuning**\n",
    "\n",
    "Performance decreases if we add untuned soft prompts, now we finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbdlDacqyHnQ"
   },
   "source": [
    "**Split datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbgEnh1oyA6l",
    "outputId": "774cc9cd-f0f2-4987-b540-0bbbfefda6de"
   },
   "outputs": [],
   "source": [
    "dataset, dataset_train, dataset_test, dataset_val = datasets.split(labels, image_emb, images, N_SAMPLES, SET_SIZES)\n",
    "batch_size = 128\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPlMtUR3eOzl",
    "outputId": "ed401195-f0c1-4d62-bf8b-bcb7c5a70f48"
   },
   "outputs": [],
   "source": [
    "model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor.feature_extractor.do_rescale = False # make sure image values: False=> [0-1] and True=> [0,255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeD46C1cglRO"
   },
   "source": [
    "## Baseline\n",
    "\n",
    "The performance of the untuned CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926
    },
    "id": "5ho8m0dqgvi4",
    "outputId": "4d032b93-868d-4887-9e99-27818de54f86"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "clip = {'m':model, 'p':processor}\n",
    "\n",
    "ft = training.FinetuneCLIP(dataloaders, clip)\n",
    "ft.tt['soft'], ft.tt['LoRA'] = 0, 0 #baseline\n",
    "\n",
    "ft.initialize({'add':''})  # do not add anything\n",
    "all_predictions, all_labels, acc = ft.eval(False)\n",
    "utils.confussion_matrix(all_labels, all_predictions, list(dataset_test.class_to_id.keys()),F1=False)\n",
    "print(f\"Accuracy of baseline is {acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieEhIC8njPgU"
   },
   "source": [
    "Predicts many as Under-nightwear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4UGkuMKf1H9"
   },
   "source": [
    "## Hard prompt tuning\n",
    "\n",
    "Easiest way to finetune, just change the text accompanied by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKhFC6XtfyiK",
    "outputId": "eb31650d-c0e5-4c02-d70f-d413fb70d434"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "clip = {'m':model, 'p':processor}\n",
    "\n",
    "texts = ['', 'An image of', 'The cloathing type is', 'An image showing cloathing of type']\n",
    "perf = []\n",
    "for added_text in texts:\n",
    "    ft = training.FinetuneCLIP(dataloaders, clip)\n",
    "    ft.tt['soft'], ft.tt['LoRA'] = 0, 0 #baseline\n",
    "    ft.train_p['add'] = added_text\n",
    "    _, _, acc = ft.eval(False)\n",
    "    perf.append(np.round(acc,2))\n",
    "print(perf)\n",
    "print(f\"Best accuracy of hard-prompt tune is {max(perf):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym-RoAfad-rV"
   },
   "source": [
    "## Soft prompt\n",
    "\n",
    "Add a tunable tensor in the embedding of the text. Added hyperparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJJ1ZeRJbT73",
    "outputId": "eeed261a-b56f-4c3b-ca09-e909b8334d0c"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "clip = {'m':model, 'p':processor}\n",
    "hp ={'hyperp':[1,2,3,4,5,6] , 'best_losses':[], 'params':[]}\n",
    "for num_soft in hp['hyperp']:\n",
    "  ft = training.FinetuneCLIP(dataloaders, clip)\n",
    "  ft.initialize({'num_soft':num_soft, 'add':''})\n",
    "  ft.tt['soft'], ft.tt['LoRA'] = 1, 0 #soft\n",
    "  _, train_p = ft.train()\n",
    "  hp['params'].append(train_p['soft']) # TODO load best of these later to evaluate on test set, I did not, i ran it again for [5].\n",
    "  hp['best_losses'].append(ft.loss['val'][-ft.es['pat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "RMyOBiwwEnrY",
    "outputId": "6a4bafcd-f33b-4668-ca22-d2b901ca938a"
   },
   "outputs": [],
   "source": [
    "plt.plot(hp['hyperp'], hp['best_losses'], marker='o', linestyle='-')\n",
    "plt.xlabel('Number of soft prompts')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ifucU86wA58A",
    "outputId": "7d3a7b2f-016e-4201-f00f-b4b921acbaf5"
   },
   "outputs": [],
   "source": [
    "ft.plot_loss()\n",
    "#utils.print_images(dataloader_train, processor,3)\n",
    "all_predictions, all_labels, acc = ft.eval(False)\n",
    "utils.confussion_matrix(all_labels, all_predictions, list(dataset_test.class_to_id.keys()),F1=False)\n",
    "print(f\"Accuracy of soft prompt is {acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMkglNVFexW4"
   },
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jLVTonuwe1R7"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "clip = {'m':model, 'p':processor}\n",
    "\n",
    "ft = training.FinetuneCLIP(dataloaders, clip)\n",
    "#ft.initialize({'num_soft':3, 'add':''}) #add rank here\n",
    "\n",
    "ft.tt['soft'], ft.tt['LoRA'] = 0, 1 #LoRA\n",
    "#loss, train_p = ft.train() # Fix initialization and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fZo65-Ofe7Tq",
    "outputId": "48832a87-6b59-4ee6-9fd5-a580ea897f93"
   },
   "outputs": [],
   "source": [
    "ft.load_p() # get best parameters\n",
    "ft.plot_loss()\n",
    "#print_images(dataset_test,2, dataloader_train)\n",
    "all_predictions, all_labels, acc = ft.eval(True)\n",
    "utils.confussion_matrix(all_labels, all_predictions, list(dataset_test.class_to_id.keys()),F1=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tyWjrmJkEeR"
   },
   "source": [
    "## Dag anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "p1hYGdZjkIL0"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "clip = {'m':model, 'p':processor}\n",
    "\n",
    "ft = training.FinetuneCLIP(dataloaders, clip)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
