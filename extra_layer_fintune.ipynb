{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samin765/DD2430_Project/blob/main/CLIP_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2INOXSFL9vte"
      },
      "source": [
        "# Constants\n",
        "Change these to fit your needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "bSDTu4h97TFF"
      },
      "outputs": [],
      "source": [
        "# Are you developing locally or on google colab?\n",
        "COLAB = False\n",
        "\n",
        "# path kaggle will download to\n",
        "HM_DATA_PATH = \"/content/drive/MyDrive/dd2430/data/\" if COLAB else \"./data/\"\n",
        "\n",
        "# path tourch.save and .load will use\n",
        "PTH_SAVE_PATH = \"/content/drive/MyDrive/dd2430/pth/\" if COLAB else \"./pth/\"\n",
        "\n",
        "# False if you have already downloaded once\n",
        "DOWNLOAD_FROM_KAGGLE = False\n",
        "\n",
        "# False if you have already created and saved a .pth file to PTH_SAVE_PATH\n",
        "CREATE_NEW_DATASET = False \n",
        "\n",
        "# train, test, val set size. Should sum to 1\n",
        "SET_SIZES = {\n",
        "    \"train\": 0.8,\n",
        "    \"test\": 0.1,\n",
        "    \"val\": 0.1,\n",
        "}\n",
        "\n",
        "# samples per class in uniform dataset\n",
        "N_SAMPLES = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJXUgcT97sz"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zPOBAjDlInvv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "if COLAB:\n",
        "    from google.colab import files, drive\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou5QvXmt3_Kr",
        "outputId": "a1292a65-1122-465c-f9c9-b891632ca9b0"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "if torch.backends.mps.is_available(): # For apple silicon \n",
        "    device = 'mps'\n",
        "\n",
        "print(\"Using device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQe6TDNGX4K"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import HMDataset2, UniformHMDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = N_SAMPLES\n",
        "file_to_load = f\"HM_data_{n_samples}.pth\"\n",
        "\n",
        "assert os.path.exists(PTH_SAVE_PATH), \"Folder with pth files does not exist\"\n",
        "\n",
        "file_path = f'{PTH_SAVE_PATH}{file_to_load}'\n",
        "assert os.path.exists(file_path), f'File {file_path} does not exist'\n",
        "\n",
        "loaded_data = torch.load(file_path)\n",
        "\n",
        "image_emb = loaded_data['image_embedding']\n",
        "labels = loaded_data['class_text']\n",
        "images = loaded_data['images']\n",
        "\n",
        "dataset = UniformHMDataset(image_emb, labels , images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAgJdAxAOLv"
      },
      "source": [
        "## Create new dataset\n",
        "This will create a new dataset and save it as a .pth to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOPl3OzJMadt",
        "outputId": "f7c7a6e8-659f-490c-8f31-813e2e992959"
      },
      "outputs": [],
      "source": [
        "if CREATE_NEW_DATASET:\n",
        "    dataset = HMDataset2(\n",
        "        articles_csv = HM_DATA_PATH + 'articles.csv',\n",
        "        image_dir = HM_DATA_PATH + 'images',\n",
        "        main_class = 'garment_group_name',\n",
        "        model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device),\n",
        "        processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    )\n",
        "\n",
        "    # data per class\n",
        "    n_samples = N_SAMPLES\n",
        "\n",
        "    # you can also set all to n_samples then set the ones you want to 0\n",
        "    for exclude_subclass in ['Unknown', 'Special Offers', 'some other']:\n",
        "        dataset.counts[exclude_subclass]=n_samples\n",
        "\n",
        "    # Create uniform dataset\n",
        "    image_emb, labels, images = dataset.get_n_of_each(n_samples)\n",
        "\n",
        "    data_to_save = {\n",
        "        'image_embedding': image_emb,\n",
        "        'class_text': labels,\n",
        "        'images': images,\n",
        "    }\n",
        "\n",
        "    os.makedirs(PTH_SAVE_PATH, exist_ok=True)\n",
        "    torch.save(data_to_save, f'{PTH_SAVE_PATH}HM_data_{n_samples}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H_a8067FDEr"
      },
      "source": [
        "# Split data into train, test, and val set\n",
        "Use `dataset_train`, `dataset_test`, and `dataset_val`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQQK-U0FFCoG",
        "outputId": "cc5f2958-f979-4f74-cc85-6604445ce78f"
      },
      "outputs": [],
      "source": [
        "# dividing the data in equal parts to the three sets\n",
        "combined = sorted(zip(labels, image_emb, images), key=lambda x: x[0])\n",
        "labels, image_emb, images = zip(*combined)\n",
        "\n",
        "train_labels, train_image_emb, train_images = [], [], []\n",
        "test_labels, test_image_emb, test_images = [], [], []\n",
        "val_labels, val_image_emb, val_images = [], [], []\n",
        "\n",
        "for i in range(0, len(combined) - 1, n_samples):\n",
        "    labels_sub = labels[i : i + n_samples]\n",
        "    image_emb_sub = image_emb[i : i + n_samples]\n",
        "    images_sub = images[i : i + n_samples]\n",
        "\n",
        "    s = lambda t: int(float(len(labels_sub)) * SET_SIZES[t])\n",
        "\n",
        "    train_labels.extend(labels_sub[:s(\"train\")])\n",
        "    train_image_emb.extend(image_emb_sub[:s(\"train\")])\n",
        "    train_images.extend(images_sub[:s(\"train\")])\n",
        "\n",
        "    test_labels.extend(labels_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "    test_image_emb.extend(image_emb_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "    test_images.extend(images_sub[s(\"train\"):s(\"train\") + s(\"test\")])\n",
        "\n",
        "    val_labels.extend(labels_sub[s(\"train\") + s(\"test\"):])\n",
        "    val_image_emb.extend(image_emb_sub[s(\"train\") + s(\"test\"):])\n",
        "    val_images.extend(images_sub[s(\"train\") + s(\"test\"):])\n",
        "\n",
        "# shuffle the data in each set\n",
        "def shuffle_set(labels, image_emb, images):\n",
        "    combined = list(zip(labels, image_emb, images))\n",
        "    random.shuffle(combined)\n",
        "    return zip(*combined)\n",
        "\n",
        "train_labels, train_image_emb, train_images = shuffle_set(train_labels, train_image_emb, train_images)\n",
        "test_labels, test_image_emb, test_images = shuffle_set(test_labels, test_image_emb, test_images)\n",
        "val_labels, val_image_emb, val_images = shuffle_set(val_labels, val_image_emb, val_images)\n",
        "\n",
        "# create the datasets\n",
        "dataset_train = UniformHMDataset(train_image_emb, train_labels, train_images)\n",
        "dataset_test = UniformHMDataset(test_image_emb, test_labels, test_images)\n",
        "dataset_val = UniformHMDataset(val_image_emb, val_labels, val_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xduBJBBGS_b"
      },
      "source": [
        "#Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fClfPP_3FRt"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5XtPmt8Po07",
        "outputId": "39ff661e-8632-4e6c-8589-cec7bb321988"
      },
      "outputs": [],
      "source": [
        "model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor.feature_extractor.do_rescale = False # make sure image values: False=> [0-1] and True=> [0,255]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions for clip text and image embeddings, forward pass etc\n",
        "# remember to import this again if you change something\n",
        "import model_functions\n",
        "# varius function that doesn't fit into model_functions or datasets\n",
        "# for example displaying images \n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7saJriyUo_J"
      },
      "source": [
        "**Baseline model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "4dNiZwM6PpuL",
        "outputId": "d7c4a276-5b35-4508-ce0a-92419ae3417b"
      },
      "outputs": [],
      "source": [
        "show_image = True\n",
        "all_predictions_baseline = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader_test)):\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "        if show_image and batch_nr % 40 == 0:\n",
        "            images = utils.return_normal(images, processor, 4, True)\n",
        "\n",
        "        predicted_class_base = logits_per_image.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_baseline.append(predicted_class_base)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "all_predictions_baseline = torch.cat(all_predictions_baseline).cpu()\n",
        "\n",
        "correct_base = all_predictions_baseline == torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy baseline {100*correct_base.sum()/correct_base.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetune "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fully connected layer to image part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a fully connected layer to the end of image model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "extra_image_layer = nn.Linear(512, 512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(extra_image_layer.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the last layer\n",
        "show_image = True\n",
        "model.train()\n",
        "loss_list = []\n",
        "epochs = 20\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(dataloader_train):\n",
        "        optimizer.zero_grad()\n",
        "        images.to(model.device)\n",
        "        text = [''+i for i in labels] # for training, must use 1-1 map\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        # text_embeds = extra_text_layer(text_embeds)\n",
        "        image_embeds = extra_image_layer(image_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model, train=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(soft_prompts.grad) to see that back prop works, is none otherwise\n",
        "        running_loss +=loss.item()\n",
        "    loss_list.append(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA1lU0jfCadS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(range(1, epochs+1)), loss_list, label='Training Loss')\n",
        "# Adding labels and title\n",
        "plt.title('Training Loss Over Datapoints')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate fully connected image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image = True\n",
        "all_predictions_image_finetune = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader_test)):\n",
        "\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        image_embeds = extra_image_layer(image_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        if show_image and batch_nr % 40 == 0:\n",
        "            images = utils.return_normal(images, processor, 4, True)\n",
        "\n",
        "        predicted_class_base = logits_per_image.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_image_finetune.append(predicted_class_base)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "all_predictions_image_finetune = torch.cat(all_predictions_image_finetune).cpu()\n",
        "\n",
        "correct_finetuned_image = all_predictions_image_finetune == torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy image_finetune {100*correct_finetuned_image.sum()/correct_finetuned_image.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fully connected to text model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "extra_text_layer = nn.Linear(512, 512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(extra_text_layer.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train last text layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the last layer\n",
        "show_image = True\n",
        "model.train()\n",
        "loss_list = []\n",
        "epochs = 20\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(dataloader_train):\n",
        "        optimizer.zero_grad()\n",
        "        images.to(model.device)\n",
        "        text = [''+i for i in labels] # for training, must use 1-1 map\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        text_embeds = extra_text_layer(text_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model, train=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(soft_prompts.grad) to see that back prop works, is none otherwise\n",
        "        running_loss +=loss.item()\n",
        "    loss_list.append(running_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate extra text layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image = True\n",
        "all_predictions_text_finetune = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader_test)):\n",
        "\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        text_embeds = extra_text_layer(text_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        if show_image and batch_nr % 40 == 0:\n",
        "            images = utils.return_normal(images, processor, 4, True)\n",
        "\n",
        "        predicted_class_base = logits_per_image.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_text_finetune.append(predicted_class_base)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "all_predictions_text_finetune = torch.cat(all_predictions_text_finetune).cpu()\n",
        "\n",
        "correct_finetuned_text = all_predictions_text_finetune == torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy text_finetune {100*correct_finetuned_text.sum()/correct_finetuned_text.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate both, trained separate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image = True\n",
        "all_predictions_both_finetune = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader_test)):\n",
        "\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        both_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        text_embeds = extra_text_layer(text_embeds)\n",
        "        image_embeds = extra_image_layer(image_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(both_embeds, image_embeds, model)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        if show_image and batch_nr % 40 == 0:\n",
        "            images = utils.return_normal(images, processor, 4, True)\n",
        "\n",
        "        predicted_class_base = logits_per_image.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_both_finetune.append(predicted_class_base)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "all_predictions_both_finetune = torch.cat(all_predictions_both_finetune).cpu()\n",
        "\n",
        "correct_finetuned_both = all_predictions_both_finetune == torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy both_finetune separate {100*correct_finetuned_both.sum()/correct_finetuned_both.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fully connected to both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "extra_image_layer2 = nn.Linear(512, 512).to(device)\n",
        "extra_text_layer2 = nn.Linear(512, 512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(list(extra_image_layer2.parameters()) + list(extra_text_layer2.parameters()), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the last layer\n",
        "show_image = True\n",
        "model.train()\n",
        "loss_list = []\n",
        "epochs = 20\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0.0\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(dataloader_train):\n",
        "        optimizer.zero_grad()\n",
        "        images.to(model.device)\n",
        "        text = [''+i for i in labels] # for training, must use 1-1 map\n",
        "        text_embeds = model_functions.get_text_emb(model, processor, text)\n",
        "        text_embeds = extra_text_layer2(text_embeds)\n",
        "        image_embeds = extra_image_layer2(image_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(text_embeds, image_embeds, model, train=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(soft_prompts.grad) to see that back prop works, is none otherwise\n",
        "        running_loss +=loss.item()\n",
        "    loss_list.append(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image = True\n",
        "all_predictions_both_fintune_same = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch_nr, (image_embeds, labels, images) in enumerate(tqdm(dataloader_test)):\n",
        "\n",
        "        text = [' '+i for i in dataset.classes] # for prediction\n",
        "        both_embeds = model_functions.get_both_emb(model, processor, text)\n",
        "        both_embeds = extra_text_layer2(both_embeds)\n",
        "        image_embeds = extra_image_layer2(image_embeds)\n",
        "        logits_per_image, loss = model_functions.apply_clip(both_embeds, image_embeds, model)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        if show_image and batch_nr % 40 == 0:\n",
        "            images = utils.return_normal(images, processor, 4, True)\n",
        "\n",
        "        predicted_class_base = logits_per_image.argmax(dim=-1)\n",
        "\n",
        "        all_predictions_both_fintune_same.append(predicted_class_base)\n",
        "        for lab in labels:\n",
        "            all_labels.append(dataset.class_to_id[lab])\n",
        "\n",
        "all_predictions_both_fintune_same = torch.cat(all_predictions_both_fintune_same).cpu()\n",
        "\n",
        "correct_fintuned_both_same = all_predictions_both_fintune_same == torch.tensor(all_labels).cpu()\n",
        "print(f'\\n Accuracy both_fintune_same {100*correct_fintuned_both_same.sum()/correct_fintuned_both_same.shape[0]} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfBohkuM3f0H"
      },
      "source": [
        "**Evaluate performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HCFdhqwG6Zy"
      },
      "outputs": [],
      "source": [
        "print(f'\\nOf {max(all_labels)} classes random is {1/max(all_labels)}%')\n",
        "print(f'Accuracy baseline {100*correct_base.sum()/correct_base.shape[0]} %')\n",
        "print(f'Accuracy image_finetune {100*correct_finetuned_image.sum()/correct_finetuned_image.shape[0]} %')\n",
        "print(f'Accuracy text_finetune {100*correct_finetuned_text.sum()/correct_finetuned_text.shape[0]} %')   \n",
        "print(f'Accuracy both_finetune separate {100*correct_finetuned_both.sum()/correct_finetuned_both.shape[0]} %')\n",
        "print(f'Accuracy both_fintune_same {100*correct_fintuned_both_same.sum()/correct_fintuned_both_same.shape[0]} %')\n",
        "from sklearn.metrics import classification_report\n",
        "print(dataset.class_to_id)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
